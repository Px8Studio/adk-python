## Orkhon local environment (do NOT commit secrets)

# Option A: Google AI API (simple API key)
# Set this to your Google AI Studio API key if you want to use the public Google AI API.
# Reference: https://ai.google.dev
#GOOGLE_API_KEY=

# Option B: Vertex AI (service account / gcloud auth)
# If you prefer Vertex AI, provide the project and region and ensure credentials are available.
# 1) With a service account key file:
#GOOGLE_APPLICATION_CREDENTIALS=C:\path\to\service-account.json
# 2) Or authenticate with gcloud ("gcloud auth application-default login") and omit the key file.

# Vertex AI runtime settings
#GOOGLE_CLOUD_PROJECT=your-gcp-project-id
#GOOGLE_CLOUD_LOCATION=us-central1

# GenAI Toolbox server (leave default for local dev)
#TOOLBOX_SERVER_URL=http://localhost:5000

# ============================================================
# BigQuery Upload Configuration
# ============================================================

# Required: GCP Project ID (same as GOOGLE_CLOUD_PROJECT if using Vertex AI)
GOOGLE_CLOUD_PROJECT=your-gcp-project-id

# GCS bucket for staging parquet files before BigQuery load
# This bucket will be created in your GCP project if it doesn't exist
GCS_BUCKET=dnb-data

# BigQuery dataset ID where tables will be created
BQ_DATASET_ID=dnb_statistics

# Optional: Field to use for time partitioning (default: period)
# Set to empty string to disable partitioning
BQ_PARTITION_FIELD=period

# Optional: Comma-separated list of fields for table clustering
# Example: BQ_CLUSTERING_FIELDS=category,subcategory
#BQ_CLUSTERING_FIELDS=

